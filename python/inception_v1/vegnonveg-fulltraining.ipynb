{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import join, basename\n",
    "import struct\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from scipy import misc\n",
    "import datetime as dt\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %pylab inline\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset import mnist\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def scala_T(input_T):\n",
    "    \"\"\"\n",
    "    Helper function for building Inception layers. Transforms a list of numbers to a dictionary with ascending keys \n",
    "    and 0 appended to the front. Ignores dictionary inputs. \n",
    "    \n",
    "    :param input_T: either list or dict\n",
    "    :return: dictionary with ascending keys and 0 appended to front {0: 0, 1: realdata_1, 2: realdata_2, ...}\n",
    "    \"\"\"    \n",
    "    if type(input_T) is list:\n",
    "        # insert 0 into first index spot, such that the real data starts from index 1\n",
    "        temp = [0]\n",
    "        temp.extend(input_T)\n",
    "        return dict(enumerate(temp))\n",
    "    # if dictionary, return it back\n",
    "    return input_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_Layer_v1(input_size, config, name_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Builds the inception-v1 submodule, a local network, that is stacked in the entire architecture when building\n",
    "    the full model.  \n",
    "    \n",
    "    :param input_size: dimensions of input coming into the local network\n",
    "    :param config: ?\n",
    "    :param name_prefix: string naming the layers of the particular local network\n",
    "    :return: concat container object with all of the Sequential layers' ouput concatenated depthwise\n",
    "    \"\"\"        \n",
    "    \n",
    "    '''\n",
    "    Concat is a container who concatenates the output of it's submodules along the provided dimension: all submodules \n",
    "    take the same inputs, and their output is concatenated.\n",
    "    '''\n",
    "    concat = Concat(2)\n",
    "    \n",
    "    \"\"\"\n",
    "    In the above code, we first create a container Sequential. Then add the layers into the container one by one. The \n",
    "    order of the layers in the model is same with the insertion order. \n",
    "    \n",
    "    \"\"\"\n",
    "    conv1 = Sequential()\n",
    "    \n",
    "    #Adding layes to the conv1 model we jus created\n",
    "    \n",
    "    #SpatialConvolution is a module that applies a 2D convolution over an input image.\n",
    "    conv1.add(SpatialConvolution(input_size, config[1][1], 1, 1, 1, 1).set_name(name_prefix + \"1x1\"))\n",
    "    conv1.add(ReLU(True).set_name(name_prefix + \"relu_1x1\"))\n",
    "    concat.add(conv1)\n",
    "    \n",
    "    conv3 = Sequential()\n",
    "    conv3.add(SpatialConvolution(input_size, config[2][1], 1, 1, 1, 1).set_name(name_prefix + \"3x3_reduce\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3_reduce\"))\n",
    "    conv3.add(SpatialConvolution(config[2][1], config[2][2], 3, 3, 1, 1, 1, 1).set_name(name_prefix + \"3x3\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3\"))\n",
    "    concat.add(conv3)\n",
    "    \n",
    "    \n",
    "    conv5 = Sequential()\n",
    "    conv5.add(SpatialConvolution(input_size,config[3][1], 1, 1, 1, 1).set_name(name_prefix + \"5x5_reduce\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5_reduce\"))\n",
    "    conv5.add(SpatialConvolution(config[3][1], config[3][2], 5, 5, 1, 1, 2, 2).set_name(name_prefix + \"5x5\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5\"))\n",
    "    concat.add(conv5)\n",
    "    \n",
    "    \n",
    "    pool = Sequential()\n",
    "    pool.add(SpatialMaxPooling(3, 3, 1, 1, 1, 1, to_ceil=True).set_name(name_prefix + \"pool\"))\n",
    "    pool.add(SpatialConvolution(input_size, config[4][1], 1, 1, 1, 1).set_name(name_prefix + \"pool_proj\"))\n",
    "    pool.add(ReLU(True).set_name(name_prefix + \"relu_pool_proj\"))\n",
    "    concat.add(pool).set_name(name_prefix + \"output\")\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_v1(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False).set_name(\"conv1/7x7_s2\"))\n",
    "    model.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool1/3x3_s2\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"pool1/norm1\"))\n",
    "    model.add(SpatialConvolution(64, 64, 1, 1, 1, 1).set_name(\"conv2/3x3_reduce\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    model.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1).set_name(\"conv2/3x3\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    model.add(Inception_Layer_v1(192, scala_T([scala_T([64]), scala_T(\n",
    "         [96, 128]), scala_T([16, 32]), scala_T([32])]), \"inception_3a/\"))\n",
    "    model.add(Inception_Layer_v1(256, scala_T([scala_T([128]), scala_T(\n",
    "         [128, 192]), scala_T([32, 96]), scala_T([64])]), \"inception_3b/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(480, scala_T([scala_T([192]), scala_T(\n",
    "         [96, 208]), scala_T([16, 48]), scala_T([64])]), \"inception_4a/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([160]), scala_T(\n",
    "         [112, 224]), scala_T([24, 64]), scala_T([64])]), \"inception_4b/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([128]), scala_T(\n",
    "         [128, 256]), scala_T([24, 64]), scala_T([64])]), \"inception_4c/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([112]), scala_T(\n",
    "         [144, 288]), scala_T([32, 64]), scala_T([64])]), \"inception_4d/\"))\n",
    "    model.add(Inception_Layer_v1(528, scala_T([scala_T([256]), scala_T(\n",
    "         [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_4e/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([256]), scala_T(\n",
    "         [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_5a/\"))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([384]), scala_T(\n",
    "         [192, 384]), scala_T([48, 128]), scala_T([128])]), \"inception_5b/\"))\n",
    "    model.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    model.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    model.add(View([1024], num_input_dims=3))\n",
    "    model.add(Linear(1024, class_num).set_name(\"loss3/classifier\"))\n",
    "    model.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "    model.reset()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the images from Amazon s3\n",
    "\n",
    "Make sure you have AWS command line interface to recursively download all images in s3 folder. You can set up aws cli from this link: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from os import path\n",
    "DATA_ROOT = \"./sample_images\"\n",
    "local_folder = DATA_ROOT + '/vegnonveg-fewsamples'\n",
    "checkpoint_path = path.join(DATA_ROOT, \"checkpoints\")\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "if not path.isdir(local_folder):\n",
    "    os.system('aws s3 cp --recursive s3://vegnonveg/vegnonveg-fewsamples %s' % local_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocess images and save them with their Labels\n",
    "\n",
    "I have saved the final dataset in a pickle file on my local machine, it can be saved into s3 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_csv = pd.read_csv(DATA_ROOT + '/vegnonveg-samples_labels.csv')\n",
    "unique_labels = labels_csv['item_name'].unique().tolist()\n",
    "label_dict = dict(zip(unique_labels, range(0,len(unique_labels))))\n",
    "class_num = len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    new_dim = min(img.width, img.height)\n",
    "    x = (img.width - new_dim) / 2\n",
    "    y = (img.height - new_dim) / 2\n",
    "    cropped = img.crop((x, y, x + new_dim, y + new_dim))\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_input = Transformer([ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                               TransposeToTensor(False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_label(labels_csv, label_dict, file_name):\n",
    "    labels = labels_csv.set_index(['obs_uid'])\n",
    "    str_label = labels.loc[file_name]['item_name']\n",
    "    return label_dict[str_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_np(image_path, IMAGE_SIZE):\n",
    "    input_img = Image.open(image_path)\n",
    "    cropped = crop_image(input_img)\n",
    "    resized = cropped.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
    "    rgb_image = resized.convert('RGB')\n",
    "    img_np = np.array(rgb_image)\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save the data as a dictionary of images and their labels, save into a pickle file\n",
    "'''\n",
    "IMAGE_SIZE = 224\n",
    "data = {}\n",
    "data['images'] = []\n",
    "data['labels'] = []\n",
    "\n",
    "files = sorted(os.listdir(local_folder))\n",
    "for file_name in files:\n",
    "    label = find_label(labels_csv, label_dict, file_name)\n",
    "    image_path = path.join(local_folder, file_name)\n",
    "    img_np = convert_to_np(image_path, IMAGE_SIZE)\n",
    "    data['images'].append(img_np)\n",
    "    data['labels'].append(label)\n",
    "\n",
    "# option to save this file into s3 bucket\n",
    "pickle.dump(data, open(DATA_ROOT + \"/processed_imgs_with_labels.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(DATA_ROOT + \"/processed_imgs_with_labels.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Stratified Train/Test Split\n",
    "To make sure we have the same distribution of samples across labels in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 2, 2)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, train_labels, test_labels = \\\n",
    "    train_test_split(data['images'], \n",
    "                     data['labels'], \n",
    "                     test_size=0.2, \n",
    "                     random_state=101)\n",
    "                     stratify=data['labels'])\n",
    "len(x_train), len(train_labels), len(x_test), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, train_counts = np.unique(np.array(train_labels), return_counts=True)\n",
    "train_counts = train_counts.astype(np.float) / len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, test_counts = np.unique(np.array(test_labels), return_counts=True)\n",
    "# test_counts = test_counts.astype(np.float) / len(test_labels)\n",
    "# # Difference in labels counts, %\n",
    "# (train_counts - test_counts) / train_counts * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_rdd_sample(images, labels):\n",
    "    \"\"\"\n",
    "    Serializes a set of images and labels for bigdl\n",
    "    \n",
    "    :param images: a list of images represented as numpy arrays\n",
    "    :param labels: a list of labels (strings) corresponding to those images \n",
    "    :return: the final dataset serialized for bigdl\n",
    "    \"\"\" \n",
    "    imgs_rdd = sc.parallelize(images)\n",
    "    labels_rdd = sc.parallelize(labels)\n",
    "    sample_rdd = imgs_rdd.zip(labels_rdd).map(lambda(img, label): Sample.from_ndarray(transform_input(img), np.array(label+1)))\n",
    "    return sample_rdd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rdd = get_rdd_sample(x_train, train_labels)\n",
    "test_rdd = get_rdd_sample(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#intializa bigdl\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.2\n",
    "# parameters for \n",
    "batch_size = 6400 #depends on dataset\n",
    "no_epochs = 2 #stop when validation accuracy doesn't improve anymore\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = len(label_dict)# item_name categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createDropout\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n"
     ]
    }
   ],
   "source": [
    "model = Inception_v1(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createClassNLLCriterion\n",
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n",
      "creating: createTrainSummary\n",
      "creating: createSeveralIteration\n",
      "creating: createValidationSummary\n",
      "saving logs to  vegnonveg\n"
     ]
    }
   ],
   "source": [
    "optimizer = Optimizer(\n",
    "    model=model,\n",
    "    training_rdd=train_rdd,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    optim_method=SGD(learningrate=learning_rate),\n",
    "    end_trigger=MaxEpoch(no_epochs),\n",
    "    batch_size=batch_size)\n",
    "# Set the validation logic\n",
    "optimizer.set_validation(\n",
    "    batch_size=batch_size,\n",
    "    val_rdd=test_rdd,\n",
    "    trigger=EveryEpoch(),\n",
    "    val_method=[Top1Accuracy()]\n",
    ")\n",
    "\n",
    "app_name= 'vegnonveg' # + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_summary = TrainSummary(log_dir='/tmp/bigdl_summaries',\n",
    "                                     app_name=app_name)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "val_summary = ValidationSummary(log_dir='/tmp/bigdl_summaries',\n",
    "                                        app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)\n",
    "print \"saving logs to \",app_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the training process,  you can run: \n",
    "```\n",
    "tensorboard --logdir=/tmp/bigdl_summaries\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "# Start to train\n",
    "trained_model = optimizer.optimize()\n",
    "print \"Optimization Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_predict_label(l):\n",
    "    return np.array(l).argmax()\n",
    "def map_groundtruth_label(l):\n",
    "    return l[0] - 1\n",
    "def map_to_label(l):\n",
    "    return label_dict.keys()[label_dict.values().index(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Ground Truth label:  Fresh oranges\n",
      "1 ) Predicted label:  Fresh carrots\n",
      "wrong\n",
      "2 ) Ground Truth label:  Fresh onions\n",
      "2 ) Predicted label:  Fresh carrots\n",
      "wrong\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Look at some predictions and their accuracy\n",
    "'''\n",
    "predictions = trained_model.predict(test_rdd)\n",
    "\n",
    "num_preds = 2\n",
    "truth = test_rdd.take(num_preds)\n",
    "preds = predictions.take(num_preds)\n",
    "\n",
    "for idx in range(num_preds):\n",
    "    true_label = str(map_to_label(map_groundtruth_label(truth[idx].label)))\n",
    "    pred_label = str(map_to_label(map_predict_label(preds[idx])))\n",
    "    print idx + 1, ')', 'Ground Truth label: ', true_label\n",
    "    print idx + 1, ')', 'Predicted label: ', pred_label\n",
    "    print \"correct\" if true_label == pred_label else \"wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTop1Accuracy\n",
      "Test result: 0.0, total_num: 2, method: Top1Accuracy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Measure Test Accuracy w/Test Set\n",
    "'''\n",
    "results = trained_model.test(test_rdd, len(x_test), [Top1Accuracy()])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
